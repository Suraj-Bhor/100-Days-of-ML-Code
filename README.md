# 100-Days-of-ML-Code
100 Days of ML Code is a commitment to better my understanding of Machine Learning by dedicating at least 1 hour of my time everyday to studying and/or coding machine learning for 100 days.

## Day 1: 9<sup>th</sup> April 2020 : Deep Neural Networks
* Watched all videos of week 4 of [Neural Networks and Deep Learning course](https://www.coursera.org/learn/neural-networks-deep-learning/home/welcome) by Prof. Andrew Ng. 
* Learnt about the forward and backward propagation along with its vectorized implementations for deep neural nets.
* Learnt about the hyperparameter tuning for complex neural nets.

## Day 2: 10<sup>th</sup> April 2020 : Implementation of deep neural net
* Completed the two programming exercises of [Neural Networks and Deep Learning course](https://www.coursera.org/learn/neural-networks-deep-learning/home/welcome).
* Implemented a 2-layer and 4-layer fully connected feed forward neural network for classifying cats vs non-cats images.
* Learnt about the vectorized implementation of entire neural net model.
* Earned a [certificate](https://www.coursera.org/account/accomplishments/records/ZS8W2LCVSHM5) for completing the course.

## Day 3: 11<sup>th</sup> April 2020 : Regularization and hyperparameter tuning.
* Started with the [Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization](https://www.coursera.org/learn/deep-neural-network/home/welcome) course taught by Prof. Andrew Ng.
* Went through the week 1 videos and learnt:
  * Splitting dataset into train, development/cross validation and test sets.
  * Bias/Variance and their tradeoff.
  * L2 regularization/Frobenius Norm and how does regularization help in reducing high variance.
  * Dropout regularization (Inverted dropout technique).
  * Data Augmentation, early stopping and other regularization techniques.
  * Also learnt about why need to normalize training sets.
* Also to better understand maths for deep learning, revised the concepts of linear algebra by watching videos of 3 Blue 1 Brown's [Essence of linear algebra](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) playlist.
* The videos give fantastic intuition about the vectors and matrix transformations and are illustrated beautifully.

## Day 4: 12<sup>th</sup> April 2020 : Implementation of different types of regularization
* Implemented different types of weights initialization methods like zero initialization, random initialization and He initialization and its effects on binary classification.
* Implemented L2 regularization and dropout regularization from scratch and understood its uses for reducing overfitting.
* Understood the concept of gradient checking and implemented it to check whether the backpropagation is computing correct gradients.
* Went through the videos of 3 Blue 1 Brown's essence of linear algebra and got to know about inituition of determinants in matrices.

## Day 5: 13<sup>th</sup> April 2020 : Optimization Algorithms
* Watched the week 2 course videos and learnt about different optimization algorithms like:
  * Mini-batch gradient descent
  * Stochastic gradient descent
  * Gradient Descent Momentum 
  * RMSprop
  * Adam Optimization Algorithm
* Learnt about exponentially weighted averages along with bias correction.
* Understood how learning decay works and also learnt how problem of local optima, saddle point and problem of plateaus occur while training deep neural networks.

## Day 6: 14<sup>th</sup> April 2020 : Implementation of Optimization Algorithms
* Implemented Mini-batch Gradient Descent, Gradient Descent Momentum and Adam Optimization Algorithm from scratch.
* Also understood the difference it makes in choosing the right optimization algorithm.

## Day 7: 15<sup>th</sup> April 2020 : Batch Normalization and Softmax Classifier
* Went through the week 3 videos of course 2 of deep learning specialization.
* Learnt about hyperparameter tuning and appropriate sampling of hyperparameters.
* Understood the concept of Batch Normalization for speeding up the learning process and also learnt how to implement batch normalization for deep neural networks.
* Also got to know how to use batch norm at testing time.
* Learnt about softmax regression and understood the loss function for softmax classifier.
* Learnt about few TensorFlow functions for implementing forward and backward pass.

## Day 8: 16<sup>th</sup> April 2020 : Iplementation of hand signs classifier in TensorFlow.
* Learnt the basics of TensorFlowv1.0. e.g placeholders, constants, variables, sessions, operations like tf.add, tf.matmul, tf.nn etc.
* Implemented a neural network using TensorFlow for classifying hand signs with accuracy of 72%.
* Completed with the course 2 of deep learning specialization and earned a [certificate](https://www.coursera.org/account/accomplishments/records/6JGSS9EHMCTV).

## Day 9: 17<sup>th</sup> April 2020 : Structuring Machine Learning Projects
* Started with the [course 3](https://www.coursera.org/learn/machine-learning-projects/home/welcome) of Deep learning specialization.
* Finished with the week 1 videos and assignment and learnt about the following concepts.
  * Orthogonalization concept.
  * Single number evaluation metric and also about satisficing and optimizing metric.
  * Splitting of Train/Dev/Test sets.
  * Comparing human-level performance to the neural network performance.
  * Avodiable bias and measure of variance.
  * Surpassing human-level performance.
  * Improving model performance.
  
  ## Day 10: 18<sup>th</sup> April 2020 : Transfer Learning and Multi-task learning
* Finished with the Structuring Machine Learning Projects course.
* Went through week 2 videos and learnt about the following concepts.
  * Carrying out error analysis.
  * Cleaning up incorrectly labeled data.
  * Training and testing on different distributions.
  * Bias and variance with mis-matched data distribution.
  * Transfer Learning.
  * Multi-task learning.
  * End-to-end Deep Learning.
* Earned course [certificate](https://www.coursera.org/account/accomplishments/records/PJJ7B368V36R).
